import os
from langchain_ollama import OllamaEmbeddings, ChatOllama # ç†è§£èˆ‡å°‹æ‰¾ã€å›ç­”èˆ‡ç¸½çµ
from langchain_chroma import Chroma # å‘é‡è³‡æ–™åº«
from langchain_core.prompts import ChatPromptTemplate # æç¤ºè©æ¨¡æ¿
from langchain_core.output_parsers import StrOutputParser # è¼¸å‡ºè§£æå™¨

# --- 1. è¨­å®šå€  ---
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "llama3.2:1b"  
PERSIST_DIRECTORY = "./db_ollama"
k_value=5

# --- 2. æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨ ---
if not os.path.exists(PERSIST_DIRECTORY):
    print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™åº«ç›®éŒ„: {PERSIST_DIRECTORY}ï¼Œè«‹å…ˆåŸ·è¡Œå»ºåº«ç¨‹å¼ã€‚")
    exit()

# --- 3. åˆå§‹åŒ–åœ°ç«¯æ¨¡å‹ ---
print(f"æ­£åœ¨è¼‰å…¥åœ°ç«¯æ¨¡å‹ ({LLM_MODEL})...")

# Embedding æ¨¡å‹ (ç”¨ä¾†æŠŠå•é¡Œè½‰æˆå‘é‡å»æœå°‹)
embeddings = OllamaEmbeddings(model=EMBED_MODEL)

# LLM æ¨¡å‹ (ç”¨ä¾†é–±è®€æœå°‹çµæœä¸¦å›ç­”)

llm = ChatOllama(
    model=LLM_MODEL, 
    temperature=0,
    num_gpu=1  # è¨­ç‚º 1 é€šå¸¸ä»£è¡¨å•Ÿç”¨ GPU åŠ é€Ÿ
)

# è¼‰å…¥ç¾æœ‰è³‡æ–™åº«
db = Chroma(
    persist_directory=PERSIST_DIRECTORY,
    embedding_function=embeddings
)

# --- 4. è¨­å®š Prompt (é‡å°åœ°ç«¯æ¨¡å‹å„ªåŒ–) ---
prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é†«ç™‚å ±å‘Šåˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„å ±å‘Šå…§å®¹ä¾†å›ç­”å•é¡Œã€‚
ç­”æ¡ˆè«‹ä½¿ç”¨ã€Œç¹é«”ä¸­æ–‡ã€å›ç­”ã€‚å¦‚æœä½ ç„¡æ³•å¾å…§å®¹ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè«‹å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦èƒ¡ç·¨äº‚é€ ã€‚

<å ±å‘Šå…§å®¹>
{context}
</å ±å‘Šå…§å®¹>

å•é¡Œï¼š{input}
""")

# å»ºç«‹åŸ·è¡Œéˆ
chain = prompt | llm | StrOutputParser()

# --- 5. åŸ·è¡Œæå• ---
query = "é€™ä»½è¨ºæ–·å ±å‘Šçš„å ±å‘Šæ’°å¯«äººæ˜¯èª°ï¼Ÿ" 

print(f"\nğŸ” æ­£åœ¨æª¢ç´¢è³‡æ–™ä¸¦ç”Ÿæˆå›ç­”...")
# æª¢ç´¢æœ€ç›¸é—œçš„ 3 å€‹ç‰‡æ®µ
docs = db.similarity_search(query, k=k_value)

# åŸ·è¡Œ
try:
    result = chain.invoke({
        "input": query, 
        "context": docs
    })
    print("\n" + "="*30)
    print(f"å•ï¼š{query}")
    print(f"ç­”ï¼š\n{result}")
    print("="*30)
except Exception as e:
    print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
    print("æç¤ºï¼šè«‹ç¢ºä¿ Ollama ä¼ºæœå™¨æ­£åœ¨åŸ·è¡Œä¸­ã€‚")
